{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njdh5Ba28jnm",
        "outputId": "5c1b06ec-08a7-4c80-d382-323f4370e5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txkB53718qiZ",
        "outputId": "edb09877-2539-40fe-f584-027388b74796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset extracted to: /content/tamil_dataset\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/ColabData/tamil_character_dataset.zip\"\n",
        "extract_path = \"/content/tamil_dataset\"\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Dataset extracted to:\", extract_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B5K5R24u8qsB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UZeGmmAJ8qvP"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),  # Scales to [0,1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Optional: Normalize to [-1, 1]\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H8ibKIKr-8X6"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/tamil_dataset/train\"\n",
        "test_dir = \"/content/tamil_dataset/test\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XyJ2aev_WS1",
        "outputId": "b059f6a2-92c7-40a2-f355-1db3bdcf0108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'க', 'கா', 'கி', 'கீ', 'கு', 'கூ', 'கெ', 'கே', 'கை', 'கொ', 'கோ', 'கௌ', 'க்', 'ங', 'ஙா', 'ஙி', 'ஙீ', 'ஙு', 'ஙூ', 'ஙெ', 'ஙே', 'ஙை', 'ஙொ', 'ஙோ', 'ஙௌ', 'ங்', 'ச', 'சா', 'சி', 'சீ', 'சு', 'சூ', 'செ', 'சே', 'சை', 'சொ', 'சோ', 'சௌ', 'ச்', 'ஞ', 'ஞா', 'ஞி', 'ஞீ', 'ஞு', 'ஞூ', 'ஞெ', 'ஞே', 'ஞை', 'ஞொ', 'ஞோ', 'ஞௌ', 'ஞ்', 'ட', 'டா', 'டி', 'டீ', 'டு', 'டூ', 'டெ', 'டே', 'டை', 'டொ', 'டோ', 'டௌ', 'ட்', 'ண', 'ணா', 'ணி', 'ணீ', 'ணு', 'ணூ', 'ணெ', 'ணே', 'ணை', 'ணொ', 'ணோ', 'ணௌ', 'ண்', 'த', 'தா', 'தி', 'தீ', 'து', 'தூ', 'தெ', 'தே', 'தை', 'தொ', 'தோ', 'தௌ', 'த்', 'ந', 'நா', 'நி', 'நீ', 'நு', 'நூ', 'நெ', 'நே', 'நை', 'நொ', 'நோ', 'நௌ', 'ந்', 'ன', 'னா', 'னி', 'னீ', 'னு', 'னூ', 'னெ', 'னே', 'னை', 'னொ', 'னோ', 'னௌ', 'ன்', 'ப', 'பா', 'பி', 'பீ', 'பு', 'பூ', 'பெ', 'பே', 'பை', 'பொ', 'போ', 'பௌ', 'ப்', 'ம', 'மா', 'மி', 'மீ', 'மு', 'மூ', 'மெ', 'மே', 'மை', 'மொ', 'மோ', 'மௌ', 'ம்', 'ய', 'யா', 'யி', 'யீ', 'யு', 'யூ', 'யெ', 'யே', 'யை', 'யொ', 'யோ', 'யௌ', 'ய்', 'ர', 'ரா', 'ரி', 'ரீ', 'ரு', 'ரூ', 'ரெ', 'ரே', 'ரை', 'ரொ', 'ரோ', 'ரௌ', 'ர்', 'ற', 'றா', 'றி', 'றீ', 'று', 'றூ', 'றெ', 'றே', 'றை', 'றொ', 'றோ', 'றௌ', 'ற்', 'ல', 'லா', 'லி', 'லீ', 'லு', 'லூ', 'லெ', 'லே', 'லை', 'லொ', 'லோ', 'லௌ', 'ல்', 'ள', 'ளா', 'ளி', 'ளீ', 'ளு', 'ளூ', 'ளெ', 'ளே', 'ளை', 'ளொ', 'ளோ', 'ளௌ', 'ள்', 'ழ', 'ழா', 'ழி', 'ழீ', 'ழு', 'ழூ', 'ழெ', 'ழே', 'ழை', 'ழொ', 'ழோ', 'ழௌ', 'ழ்', 'வ', 'வா', 'வி', 'வீ', 'வு', 'வூ', 'வெ', 'வே', 'வை', 'வொ', 'வோ', 'வௌ', 'வ்']\n"
          ]
        }
      ],
      "source": [
        "print(\"Classes:\", train_dataset.classes)\n",
        "num_classes = len(train_dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwTYGqeK_d0Q"
      },
      "source": [
        "CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M0Oyo9O2_WZ7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> [B, 32, 32, 32]\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> [B, 64, 16, 16]\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq3e3zQF_WcS",
        "outputId": "a382542b-54fe-4f2f-ad8f-9e7272ecde3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Epoch [1/10], Loss: 545.1942\n",
            "📘 Epoch [2/10], Loss: 31.8725\n",
            "📘 Epoch [3/10], Loss: 17.9415\n",
            "📘 Epoch [4/10], Loss: 12.7143\n",
            "📘 Epoch [5/10], Loss: 11.4501\n",
            "📘 Epoch [6/10], Loss: 8.7284\n",
            "📘 Epoch [7/10], Loss: 6.4908\n",
            "📘 Epoch [8/10], Loss: 5.8999\n",
            "📘 Epoch [9/10], Loss: 7.0241\n",
            "📘 Epoch [10/10], Loss: 8.8369\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNClassifier(num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"📘 Epoch [{epoch+1}/10], Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb5wsZVb_Wfw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"vit_tamil_model.pth\")\n"
      ],
      "metadata": {
        "id": "FusP-AV-pSR6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Define CNN architecture (same as your training)\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # [B, 32, 32, 32]\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # [B, 64, 16, 16]\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 🔧 Preprocessing function for raw image\n",
        "def clean_image_pil(pil_img, canvas_size=64, char_size=40):\n",
        "    # Convert to grayscale numpy array\n",
        "    img = np.array(pil_img.convert(\"L\"))\n",
        "\n",
        "    # Invert if white background\n",
        "    if np.mean(img) > 127:\n",
        "        img = cv2.bitwise_not(img)\n",
        "\n",
        "    # Binarize\n",
        "    _, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Crop to bounding box\n",
        "    coords = cv2.findNonZero(binary)\n",
        "    x, y, w, h = cv2.boundingRect(coords)\n",
        "    cropped = binary[y:y+h, x:x+w]\n",
        "\n",
        "    # Resize character\n",
        "    resized = cv2.resize(cropped, (char_size, char_size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Place on black canvas\n",
        "    canvas = np.zeros((canvas_size, canvas_size), dtype=np.uint8)\n",
        "    start_x = (canvas_size - char_size) // 2\n",
        "    start_y = (canvas_size - char_size) // 2\n",
        "    canvas[start_y:start_y+char_size, start_x:start_x+char_size] = resized\n",
        "\n",
        "    # Ensure white on black\n",
        "    if np.mean(canvas) > 127:\n",
        "        canvas = cv2.bitwise_not(canvas)\n",
        "\n",
        "    return canvas\n",
        "\n",
        "# Load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "label_classes = train_dataset.classes  # Make sure this is loaded from your dataset\n",
        "num_classes = len(label_classes)\n",
        "\n",
        "model = CNNClassifier(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(\"vit_tamil_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Upload raw image\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "raw_img = Image.open(io.BytesIO(uploaded[image_path]))\n",
        "\n",
        "# Clean the image\n",
        "cleaned = clean_image_pil(raw_img)\n",
        "\n",
        "# Show cleaned image\n",
        "plt.imshow(cleaned, cmap='gray')\n",
        "plt.title(\"Preprocessed Input\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Prepare for model input\n",
        "img_tensor = torch.tensor(cleaned, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "img_tensor = (img_tensor - 0.5) / 0.5  # normalize\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "    pred_idx = output.argmax(dim=1).item()\n",
        "    confidence = F.softmax(output, dim=1)[0][pred_idx].item() * 100\n",
        "    pred_label = label_classes[pred_idx]\n",
        "\n",
        "# ✅ Final Output\n",
        "print(f\"\\n🧠 Predicted Tamil Character: **{pred_label}**\")\n",
        "print(f\"🔍 Confidence: {confidence:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Hm6SSBAJrW31"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}