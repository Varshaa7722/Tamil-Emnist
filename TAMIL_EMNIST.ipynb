{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njdh5Ba28jnm",
        "outputId": "5c1b06ec-08a7-4c80-d382-323f4370e5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txkB53718qiZ",
        "outputId": "edb09877-2539-40fe-f584-027388b74796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset extracted to: /content/tamil_dataset\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/ColabData/tamil_character_dataset.zip\"\n",
        "extract_path = \"/content/tamil_dataset\"\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"‚úÖ Dataset extracted to:\", extract_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B5K5R24u8qsB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UZeGmmAJ8qvP"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),  # Scales to [0,1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Optional: Normalize to [-1, 1]\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H8ibKIKr-8X6"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/tamil_dataset/train\"\n",
        "test_dir = \"/content/tamil_dataset/test\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XyJ2aev_WS1",
        "outputId": "b059f6a2-92c7-40a2-f355-1db3bdcf0108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['‡ÆÉ', '‡ÆÖ', '‡ÆÜ', '‡Æá', '‡Æà', '‡Æâ', '‡Æä', '‡Æé', '‡Æè', '‡Æê', '‡Æí', '‡Æì', '‡Æî', '‡Æï', '‡Æï‡Ææ', '‡Æï‡Æø', '‡Æï‡ØÄ', '‡Æï‡ØÅ', '‡Æï‡ØÇ', '‡Æï‡ØÜ', '‡Æï‡Øá', '‡Æï‡Øà', '‡Æï‡Øä', '‡Æï‡Øã', '‡Æï‡Øå', '‡Æï‡Øç', '‡Æô', '‡Æô‡Ææ', '‡Æô‡Æø', '‡Æô‡ØÄ', '‡Æô‡ØÅ', '‡Æô‡ØÇ', '‡Æô‡ØÜ', '‡Æô‡Øá', '‡Æô‡Øà', '‡Æô‡Øä', '‡Æô‡Øã', '‡Æô‡Øå', '‡Æô‡Øç', '‡Æö', '‡Æö‡Ææ', '‡Æö‡Æø', '‡Æö‡ØÄ', '‡Æö‡ØÅ', '‡Æö‡ØÇ', '‡Æö‡ØÜ', '‡Æö‡Øá', '‡Æö‡Øà', '‡Æö‡Øä', '‡Æö‡Øã', '‡Æö‡Øå', '‡Æö‡Øç', '‡Æû', '‡Æû‡Ææ', '‡Æû‡Æø', '‡Æû‡ØÄ', '‡Æû‡ØÅ', '‡Æû‡ØÇ', '‡Æû‡ØÜ', '‡Æû‡Øá', '‡Æû‡Øà', '‡Æû‡Øä', '‡Æû‡Øã', '‡Æû‡Øå', '‡Æû‡Øç', '‡Æü', '‡Æü‡Ææ', '‡Æü‡Æø', '‡Æü‡ØÄ', '‡Æü‡ØÅ', '‡Æü‡ØÇ', '‡Æü‡ØÜ', '‡Æü‡Øá', '‡Æü‡Øà', '‡Æü‡Øä', '‡Æü‡Øã', '‡Æü‡Øå', '‡Æü‡Øç', '‡Æ£', '‡Æ£‡Ææ', '‡Æ£‡Æø', '‡Æ£‡ØÄ', '‡Æ£‡ØÅ', '‡Æ£‡ØÇ', '‡Æ£‡ØÜ', '‡Æ£‡Øá', '‡Æ£‡Øà', '‡Æ£‡Øä', '‡Æ£‡Øã', '‡Æ£‡Øå', '‡Æ£‡Øç', '‡Æ§', '‡Æ§‡Ææ', '‡Æ§‡Æø', '‡Æ§‡ØÄ', '‡Æ§‡ØÅ', '‡Æ§‡ØÇ', '‡Æ§‡ØÜ', '‡Æ§‡Øá', '‡Æ§‡Øà', '‡Æ§‡Øä', '‡Æ§‡Øã', '‡Æ§‡Øå', '‡Æ§‡Øç', '‡Æ®', '‡Æ®‡Ææ', '‡Æ®‡Æø', '‡Æ®‡ØÄ', '‡Æ®‡ØÅ', '‡Æ®‡ØÇ', '‡Æ®‡ØÜ', '‡Æ®‡Øá', '‡Æ®‡Øà', '‡Æ®‡Øä', '‡Æ®‡Øã', '‡Æ®‡Øå', '‡Æ®‡Øç', '‡Æ©', '‡Æ©‡Ææ', '‡Æ©‡Æø', '‡Æ©‡ØÄ', '‡Æ©‡ØÅ', '‡Æ©‡ØÇ', '‡Æ©‡ØÜ', '‡Æ©‡Øá', '‡Æ©‡Øà', '‡Æ©‡Øä', '‡Æ©‡Øã', '‡Æ©‡Øå', '‡Æ©‡Øç', '‡Æ™', '‡Æ™‡Ææ', '‡Æ™‡Æø', '‡Æ™‡ØÄ', '‡Æ™‡ØÅ', '‡Æ™‡ØÇ', '‡Æ™‡ØÜ', '‡Æ™‡Øá', '‡Æ™‡Øà', '‡Æ™‡Øä', '‡Æ™‡Øã', '‡Æ™‡Øå', '‡Æ™‡Øç', '‡ÆÆ', '‡ÆÆ‡Ææ', '‡ÆÆ‡Æø', '‡ÆÆ‡ØÄ', '‡ÆÆ‡ØÅ', '‡ÆÆ‡ØÇ', '‡ÆÆ‡ØÜ', '‡ÆÆ‡Øá', '‡ÆÆ‡Øà', '‡ÆÆ‡Øä', '‡ÆÆ‡Øã', '‡ÆÆ‡Øå', '‡ÆÆ‡Øç', '‡ÆØ', '‡ÆØ‡Ææ', '‡ÆØ‡Æø', '‡ÆØ‡ØÄ', '‡ÆØ‡ØÅ', '‡ÆØ‡ØÇ', '‡ÆØ‡ØÜ', '‡ÆØ‡Øá', '‡ÆØ‡Øà', '‡ÆØ‡Øä', '‡ÆØ‡Øã', '‡ÆØ‡Øå', '‡ÆØ‡Øç', '‡Æ∞', '‡Æ∞‡Ææ', '‡Æ∞‡Æø', '‡Æ∞‡ØÄ', '‡Æ∞‡ØÅ', '‡Æ∞‡ØÇ', '‡Æ∞‡ØÜ', '‡Æ∞‡Øá', '‡Æ∞‡Øà', '‡Æ∞‡Øä', '‡Æ∞‡Øã', '‡Æ∞‡Øå', '‡Æ∞‡Øç', '‡Æ±', '‡Æ±‡Ææ', '‡Æ±‡Æø', '‡Æ±‡ØÄ', '‡Æ±‡ØÅ', '‡Æ±‡ØÇ', '‡Æ±‡ØÜ', '‡Æ±‡Øá', '‡Æ±‡Øà', '‡Æ±‡Øä', '‡Æ±‡Øã', '‡Æ±‡Øå', '‡Æ±‡Øç', '‡Æ≤', '‡Æ≤‡Ææ', '‡Æ≤‡Æø', '‡Æ≤‡ØÄ', '‡Æ≤‡ØÅ', '‡Æ≤‡ØÇ', '‡Æ≤‡ØÜ', '‡Æ≤‡Øá', '‡Æ≤‡Øà', '‡Æ≤‡Øä', '‡Æ≤‡Øã', '‡Æ≤‡Øå', '‡Æ≤‡Øç', '‡Æ≥', '‡Æ≥‡Ææ', '‡Æ≥‡Æø', '‡Æ≥‡ØÄ', '‡Æ≥‡ØÅ', '‡Æ≥‡ØÇ', '‡Æ≥‡ØÜ', '‡Æ≥‡Øá', '‡Æ≥‡Øà', '‡Æ≥‡Øä', '‡Æ≥‡Øã', '‡Æ≥‡Øå', '‡Æ≥‡Øç', '‡Æ¥', '‡Æ¥‡Ææ', '‡Æ¥‡Æø', '‡Æ¥‡ØÄ', '‡Æ¥‡ØÅ', '‡Æ¥‡ØÇ', '‡Æ¥‡ØÜ', '‡Æ¥‡Øá', '‡Æ¥‡Øà', '‡Æ¥‡Øä', '‡Æ¥‡Øã', '‡Æ¥‡Øå', '‡Æ¥‡Øç', '‡Æµ', '‡Æµ‡Ææ', '‡Æµ‡Æø', '‡Æµ‡ØÄ', '‡Æµ‡ØÅ', '‡Æµ‡ØÇ', '‡Æµ‡ØÜ', '‡Æµ‡Øá', '‡Æµ‡Øà', '‡Æµ‡Øä', '‡Æµ‡Øã', '‡Æµ‡Øå', '‡Æµ‡Øç']\n"
          ]
        }
      ],
      "source": [
        "print(\"Classes:\", train_dataset.classes)\n",
        "num_classes = len(train_dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwTYGqeK_d0Q"
      },
      "source": [
        "CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M0Oyo9O2_WZ7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> [B, 32, 32, 32]\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> [B, 64, 16, 16]\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq3e3zQF_WcS",
        "outputId": "a382542b-54fe-4f2f-ad8f-9e7272ecde3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìò Epoch [1/10], Loss: 545.1942\n",
            "üìò Epoch [2/10], Loss: 31.8725\n",
            "üìò Epoch [3/10], Loss: 17.9415\n",
            "üìò Epoch [4/10], Loss: 12.7143\n",
            "üìò Epoch [5/10], Loss: 11.4501\n",
            "üìò Epoch [6/10], Loss: 8.7284\n",
            "üìò Epoch [7/10], Loss: 6.4908\n",
            "üìò Epoch [8/10], Loss: 5.8999\n",
            "üìò Epoch [9/10], Loss: 7.0241\n",
            "üìò Epoch [10/10], Loss: 8.8369\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNClassifier(num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"üìò Epoch [{epoch+1}/10], Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb5wsZVb_Wfw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"vit_tamil_model.pth\")\n"
      ],
      "metadata": {
        "id": "FusP-AV-pSR6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Define CNN architecture (same as your training)\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # [B, 32, 32, 32]\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # [B, 64, 16, 16]\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# üîß Preprocessing function for raw image\n",
        "def clean_image_pil(pil_img, canvas_size=64, char_size=40):\n",
        "    # Convert to grayscale numpy array\n",
        "    img = np.array(pil_img.convert(\"L\"))\n",
        "\n",
        "    # Invert if white background\n",
        "    if np.mean(img) > 127:\n",
        "        img = cv2.bitwise_not(img)\n",
        "\n",
        "    # Binarize\n",
        "    _, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Crop to bounding box\n",
        "    coords = cv2.findNonZero(binary)\n",
        "    x, y, w, h = cv2.boundingRect(coords)\n",
        "    cropped = binary[y:y+h, x:x+w]\n",
        "\n",
        "    # Resize character\n",
        "    resized = cv2.resize(cropped, (char_size, char_size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Place on black canvas\n",
        "    canvas = np.zeros((canvas_size, canvas_size), dtype=np.uint8)\n",
        "    start_x = (canvas_size - char_size) // 2\n",
        "    start_y = (canvas_size - char_size) // 2\n",
        "    canvas[start_y:start_y+char_size, start_x:start_x+char_size] = resized\n",
        "\n",
        "    # Ensure white on black\n",
        "    if np.mean(canvas) > 127:\n",
        "        canvas = cv2.bitwise_not(canvas)\n",
        "\n",
        "    return canvas\n",
        "\n",
        "# Load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "label_classes = train_dataset.classes  # Make sure this is loaded from your dataset\n",
        "num_classes = len(label_classes)\n",
        "\n",
        "model = CNNClassifier(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(\"vit_tamil_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Upload raw image\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "raw_img = Image.open(io.BytesIO(uploaded[image_path]))\n",
        "\n",
        "# Clean the image\n",
        "cleaned = clean_image_pil(raw_img)\n",
        "\n",
        "# Show cleaned image\n",
        "plt.imshow(cleaned, cmap='gray')\n",
        "plt.title(\"Preprocessed Input\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Prepare for model input\n",
        "img_tensor = torch.tensor(cleaned, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "img_tensor = (img_tensor - 0.5) / 0.5  # normalize\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "    pred_idx = output.argmax(dim=1).item()\n",
        "    confidence = F.softmax(output, dim=1)[0][pred_idx].item() * 100\n",
        "    pred_label = label_classes[pred_idx]\n",
        "\n",
        "# ‚úÖ Final Output\n",
        "print(f\"\\nüß† Predicted Tamil Character: **{pred_label}**\")\n",
        "print(f\"üîç Confidence: {confidence:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Hm6SSBAJrW31"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}